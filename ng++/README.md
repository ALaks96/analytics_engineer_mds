# ğŸ—ï¸ NG++ : The Scale-Up Challenge (E-commerce & Performance)

ğŸš¨ **Niveau : Expert.**

Vous savez faire des pipelines qui marchent. Maintenant, on va faire des pipelines qui **tiennent la charge**.
Dans ce projet, vous n'Ãªtes plus un stagiaire. Vous Ãªtes le Lead Analytics Engineer.

**Le scÃ©nario :**
Votre site E-commerce "TheLook" explose.
Votre Data Warehouse commence Ã  ramer.
Votre CFO gueule parce que la facture BigQuery a doublÃ©.
Votre mission : Refondre les modÃ¨les pour gÃ©rer l'**IncrÃ©mentalitÃ©** et rÃ©pondre Ã  des questions business complexes.

---

## ğŸ’¾ La Source de DonnÃ©es

Pas besoin d'ingestion ici, la donnÃ©e est dÃ©jÃ  dans BigQuery (Public Datasets).
* **Projet :** `bigquery-public-data`
* **Dataset :** `thelook_ecommerce`
* **Tables clÃ©s :** `orders`, `order_items`, `products`, `users`, `events`.

---

## ğŸ§± Partie 1 : Complex Modeling (L'OBT)

Le marketing ne veut pas faire 4 jointures Ã  chaque fois qu'ils veulent un chiffre. Ils veulent une **One Big Table (OBT)**.

### ğŸ¯ Mission Business
CrÃ©er un modÃ¨le **Facts** (`fct_orders_details`) ultra-complet qui rÃ©pond Ã  ces questions sans refaire de jointures :
1.  Quel est le panier moyen par catÃ©gorie de produit ?
2.  Quelle est la marge brute par commande ? (`prix de vente - coÃ»t de production`).
3.  Combien de temps s'Ã©coule entre la crÃ©ation du compte user et sa premiÃ¨re commande ?

### ğŸ› ï¸ Le DÃ©fi Technique
Vous devez joindre : `orders` + `order_items` + `products` + `users`.
* *Attention :* Une commande peut avoir plusieurs items. La granularitÃ© de votre table finale doit Ãªtre `order_item_id` (une ligne par article achetÃ©), mais enrichie avec les infos de la commande et de l'utilisateur.

---

## âš¡ Partie 2 : L'IncrÃ©mentalitÃ© (Le cÅ“ur du sujet)

C'est lÃ  que Ã§a se corse. La table `events` (clics sur le site) contient des millions de lignes.
Si vous faites un `DROP TABLE` + `CREATE TABLE` (Full Refresh) tous les matins, Ã§a prend 1 heure.

### ğŸ§  Le Concept : Incremental Models
Au lieu de tout recalculer, on ne traite que **ce qui s'est passÃ© depuis la derniÃ¨re fois**.
* **Jour J :** On charge tout l'historique.
* **Jour J+1 :** On charge uniquement les Ã©vÃ©nements d'hier et on les "colle" (Append) Ã  la suite.



### ğŸ¯ Mission : Setup Incremental
1.  Configurez votre modÃ¨le dbt `fct_events` en `materialized='incremental'`.
2.  Utilisez la logique Jinja `{% if is_incremental() %}` pour filtrer uniquement les nouvelles lignes.
    * *Condition :* Charger les lignes oÃ¹ `created_at` est supÃ©rieur Ã  la date max dÃ©jÃ  prÃ©sente dans la table.

### ğŸ§ª Le Test de vÃ©ritÃ© (Simulation)
Comme la donnÃ©e publique est statique, vous allez simuler l'avancÃ©e du temps.
1.  Lancez dbt en filtrant les donnÃ©es avant 2023 : `dbt run --vars 'max_date: 2023-01-01'` (il faudra coder cette variable dans le WHERE de votre SQL).
2.  VÃ©rifiez le nombre de lignes.
3.  Relancez dbt avec `2023-02-01`.
4.  VÃ©rifiez que seules les lignes de janvier ont Ã©tÃ© ajoutÃ©es (regardez les logs BigQuery "Bytes processed", Ã§a doit Ãªtre minuscule).

---

## ğŸ”„ Partie 3 : Gestion des Updates (Merge & Unique Key)

L'incrÃ©mental "Append-only" (ajout simple), c'est facile. Mais dans le E-commerce, les commandes changent de statut !
Une commande passe de `Processing` Ã  `Shipped` puis potentiellement Ã  `Returned`.

Si vous faites juste "Append", vous aurez deux lignes pour la mÃªme commande (une en processing, une en shipped). C'est faux.
Il faut faire un **Merge** (Mise Ã  jour).

### ğŸ¯ Mission : Handle Returns
1.  Modifiez votre modÃ¨le `fct_orders`.
2.  Configurez la `unique_key` dans dbt.
3.  Le but : Si l'ID de commande existe dÃ©jÃ , on met Ã  jour la ligne (pour changer le statut). Si elle n'existe pas, on l'ajoute.

### ğŸ§  Le Concept : Merge vs Append
* **Append :** J'ajoute les nouvelles lignes au fond du fichier. Rapide, mais ne gÃ¨re pas les modifications du passÃ©.
* **Merge :** Je compare les nouvelles lignes avec les anciennes. Si l'ID matche, j'Ã©crase (UPDATE). Sinon, j'ajoute (INSERT). C'est plus lourd, mais c'est exact.

---

## ğŸ•µï¸ Partie 4 : Le Snapshotting (SCD Type 2)

Votre boss vous demande : *"Est-ce que les utilisateurs qui habitent Ã  Paris achÃ¨tent plus cher ?"*
Facile, vous regardez l'adresse.
MAIS, si un utilisateur dÃ©mÃ©nage de Paris Ã  Lyon, son adresse change dans la base.
Si vous analysez ses commandes de l'an dernier (quand il Ã©tait Ã  Paris) avec son adresse d'aujourd'hui (Lyon), votre analyse est fausse.

Il faut historiser les changements d'adresse.

### ğŸ§  Le Concept : Slowly Changing Dimensions (SCD)
* **Type 1 :** On Ã©crase l'ancienne valeur (On perd l'historique).
* **Type 2 :** On crÃ©e une nouvelle ligne avec une date de dÃ©but et de fin (`valid_from`, `valid_to`).

### ğŸ¯ Mission
1.  CrÃ©er un "Snapshot" dbt sur la table `users`.
2.  StratÃ©gie : `timestamp` (basÃ© sur la colonne de mise Ã  jour) ou `check` (si vous voulez surveiller une colonne spÃ©cifique comme `city`).
3.  Lancez le snapshot (`dbt snapshot`).
4.  Observez comment dbt crÃ©e automatiquement les colonnes `dbt_valid_from` et `dbt_valid_to`.

---

## ğŸ† Boss Final : La Question PiÃ¨ge

Si vous avez rÃ©ussi tout Ã§a, rÃ©pondez Ã  cette question business en SQL en utilisant vos modÃ¨les :

> "Calculez le taux de retour (Refund Rate) par Cohorte mensuelle d'inscription.
> Est-ce que les utilisateurs inscrits en Janvier retournent plus leurs colis que ceux inscrits en Juin ?"

*Indices pour rÃ©ussir :*
* Il faut prendre la date de crÃ©ation de l'user (Cohort).
* Il faut regarder le statut de ses commandes (Returned).
* Attention : Un user inscrit en Janvier peut commander en Mars. Le retour compte pour la cohorte Janvier.

Bon courage. Si vous savez gÃ©rer l'incrÃ©mentalitÃ© et les snapshots, vous Ãªtes techniquement au-dessus de 80% des juniors.