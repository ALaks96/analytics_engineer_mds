# ü¶Ö Projet Solo : Le Radar du March√© Data

üö® **Attention : Mode Tutoriel d√©sactiv√©.**

Vous avez termin√© le projet V√©lib'. Vous savez connecter des briques (API -> Python -> BQ -> dbt -> Viz). C'est bien.
Maintenant, on passe au niveau r√©el. Pas de cheat codes, pas de code √† copier-coller. Juste une documentation officielle et votre cerveau.

---

## üéØ L'Objectif Business

Vous cherchez √† vous reconvertir dans la Data.
**Votre mission :** Construire un outil qui analyse en temps r√©el le march√© de l'emploi Data en France via l'API officielle de France Travail (ex-P√¥le Emploi).

**Les questions auxquelles votre Dashboard doit r√©pondre :**
1.  Quel est le salaire moyen *r√©el* mentionn√© dans les offres "Data Analyst" vs "Data Engineer" ?
2.  Quelles sont les comp√©tences techniques (Hard Skills) les plus demand√©es ? (SQL apparait-il plus souvent que Python ?)
3.  O√π sont les jobs ? (Paris vs Reste de la France).

---

## ‚öôÔ∏è La Stack Technique (Impos√©e)

* **Source :** API "Offres d'emploi v2" (France Travail).
* **Ingestion :** Python (Script avec gestion d'authentification OAuth2 & Pagination).
* **Storage :** BigQuery.
* **Transformation :** dbt (Gros focus sur le nettoyage de texte et les Regex).
* **Viz :** Looker Studio.

---

## üíÄ Les 3 Boss Techniques du projet

Ce projet est difficile √† cause de ces trois points. Si vous les passez, vous √™tes techniquement pr√™ts pour un job.

### 1. L'Authentification OAuth2 (Python)
L'API France Travail n'est pas ouverte comme celle des V√©lib'. Elle est prot√©g√©e.
Vous ne pouvez pas juste faire un `requests.get(url)`.
Il faut impl√©menter le protocole **OAuth2 (Client Credentials)** :
1.  Envoyer votre `client_id` et `client_secret` √† une URL d'authentification.
2.  R√©cup√©rer un "Token" (jeton) temporaire.
3.  Utiliser ce Token pour interroger l'API des offres.
4.  G√©rer l'expiration du Token (il ne dure que 20 minutes !).

### 2. La Pagination (Python)
L'API ne vous donnera pas toutes les offres d'un coup. Elle donne les 150 premi√®res.
Vous devez √©crire une **boucle** en Python qui :
* R√©cup√®re la page 1.
* V√©rifie s'il y a une suite.
* R√©cup√®re la page 2, etc.
* S'arr√™te quand il n'y a plus rien.

### 3. Le Parsing de Texte & Regex (SQL/dbt)
Le champ "Salaire" dans les offres est un champ texte libre rempli par des humains.
Exemples de ce que vous allez recevoir :
* *"35k √† 45k selon profil"*
* *"Annuel de 30000.0 Euros √† 40000.0 Euros sur 12.0 mois"*
* *"Selon exp√©rience"*

Vous allez devoir utiliser des **Regular Expressions (Regex)** dans dbt pour extraire les chiffres et calculer une moyenne fiable.

---

## üó∫Ô∏è Roadmap & Ressources

### √âtape 1 : Obtenir les cl√©s du camion
Il faut cr√©er un compte d√©veloppeur sur la plateforme de l'√âtat.
* **Site :** [France Travail / Emploi Store Dev](https://www.emploi-store-dev.fr/portail-developpeur)
* Cr√©ez un compte.
* Abonnez-vous √† l'API **"Offres d'emploi v2"**.
* R√©cup√©rez vos `Client ID` et `Client Secret`.

### √âtape 2 : Le Script d'Extraction (Python)
C'est l'√©tape la plus dure.
* Documentation officielle de l'API : [Lire la doc (Swagger)](https://www.emploi-store-dev.fr/portail-developpeur/detail-api/offres-d-emploi-v2)
* Indice : Cherchez "Python requests OAuth2 client credentials example" sur Google.

### √âtape 3 : Stockage (BigQuery)
Envoyez le JSON brut dans une table `raw_jobs`.
* *Conseil :* Ajoutez une colonne `ingestion_date` pour pouvoir suivre l'√©volution du march√© jour apr√®s jour.

### √âtape 4 : Le Nettoyage (dbt)
Cr√©ez vos mod√®les `staging`.
* **Challenge SQL :** Le champ `competences` est souvent une liste imbriqu√©e (Array) dans le JSON. Vous allez devoir utiliser la fonction `UNNEST()` de BigQuery pour "aplatir" cette liste et compter les comp√©tences.
* **Challenge SQL :** Utilisez `REGEXP_EXTRACT` pour trouver les salaires.

### √âtape 5 : La Viz
Faites parler les donn√©es.
* Un graphique barre : Top 10 des comp√©tences demand√©es.
* Une carte : Densit√© des offres.

---

## üèÜ Definition of Done

Le projet est valid√© si :
1.  Vous pouvez lancer le script Python et il r√©cup√®re **toutes** les offres (pas juste 150) contenant le mot cl√© "Data".
2.  Vous avez une table propre dans BigQuery avec une colonne `salaire_moyen_estime` (type FLOAT/INT) et non du texte.
3.  Vous avez un dashboard qui montre quel langage (Python, R, SQL, Java) paye le mieux.

Bonne chance. Google est votre meilleur ami.