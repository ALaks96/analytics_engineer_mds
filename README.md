# ğŸš´â€â™‚ï¸ Projet : The Paris VÃ©lib' Monitor

Bienvenue. Si vous lisez ceci, c'est que vous envisagez de passer du cÃ´tÃ© technique de la Data ("Analytics Engineering").

Ce projet est conÃ§u comme un jeu vidÃ©o en 2 Phases. Il reproduit **exactement** ce que font les ingÃ©nieurs data en startup, mais gratuitement.

* **Phase 1 (Le MVP) :** On fait le pipeline "Ã  la main" pour comprendre la logique (SQL, ModÃ©lisation, Dashboard).
* **Phase 2 (L'Industrialisation) :** On automatise tout avec du code et des outils d'infrastructure (Python, Docker, Airbyte).

---

## ğŸ›  PrÃ©-requis & Outils

Avant de commencer, installez ces bases. C'est votre trousse Ã  outils.
* **VS Code** (Votre atelier de code) : [TÃ©lÃ©charger ici](https://code.visualstudio.com/)
* **Git** (Votre sauvegarde) : [TÃ©lÃ©charger ici](https://git-scm.com/downloads)
* **Un compte Google** (Pour accÃ©der au Cloud).

---

# ğŸ Phase 1 : Le MVP (Minimum Viable Product)

**Objectif :** Sortir un tableau de bord fonctionnel le plus vite possible, mÃªme si la mÃ©thode d'ingestion est "bricolÃ©e" (manuelle).

## Ã‰tape 1 : Le Cloud (BigQuery)

Vous n'allez pas stocker les donnÃ©es sur votre ordi (Excel), mais dans un "Data Warehouse" (EntrepÃ´t de donnÃ©es).

### ğŸ§  C'est quoi le concept ?
Un **Data Warehouse** (comme BigQuery ou Snowflake) est une base de donnÃ©es sur-vitaminÃ©e.
* Contrairement Ã  Excel, elle peut stocker des pÃ©taoctets de donnÃ©es.
* Elle sÃ©pare le stockage (disque) du calcul (processeur), ce qui permet de lancer des requÃªtes trÃ¨s lourdes en quelques secondes.
* C'est ici que **toute** la donnÃ©e de l'entreprise atterrit.

### ğŸ¯ Mission
1.  Aller sur la **Google Cloud Platform (GCP)** et crÃ©er un nouveau projet.
2.  Chercher "BigQuery" dans la barre de recherche.
3.  CrÃ©er un "Dataset" (un dossier) appelÃ© `raw_velib`.
4.  TÃ©lÃ©charger la donnÃ©e brute (un fichier JSON) sur votre ordi : [Data VÃ©lib Temps RÃ©el](https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json)
5.  CrÃ©er une table `stations` dans votre dataset `raw_velib` en uploadant ce fichier JSON manuellement.

### ğŸ”— Liens utiles
* **Tuto Indispensable :** [Comment activer la BigQuery Sandbox (Gratuit)](https://cloud.google.com/bigquery/docs/sandbox?hl=fr)
* **Tuto :** [Charger un fichier local dans BigQuery](https://cloud.google.com/bigquery/docs/loading-data-local?hl=fr) (Regardez la partie "Console Cloud").

---

## Ã‰tape 2 : La Transformation (dbt & SQL)

C'est le cÅ“ur du mÃ©tier d'Analytics Engineer. Votre donnÃ©e dans BigQuery est brute, sale et inutilisable par le mÃ©tier.

### ğŸ§  C'est quoi le concept ?
On ne modifie jamais la source (`raw`). On crÃ©e une copie propre.
**"Nettoyer la donnÃ©e"** en SQL, Ã§a veut dire prÃ©cisÃ©ment 3 choses :
1.  **Aliasing (Renommer) :** `num_bikes_available` c'est moche. On renomme en `nb_velos`.
2.  **Casting (Typer) :** Parfois un chiffre arrive comme du texte `"42"`. On doit le forcer Ã  devenir un nombre `42` pour faire des additions.
3.  **Filtrage :** Enlever les lignes de test ou les erreurs.

On utilise **dbt (data build tool)** pour organiser ce SQL. C'est le standard du marchÃ©.


### ğŸ¯ Mission
1.  Installer dbt sur votre ordinateur (via le terminal).
2.  Configurer dbt pour qu'il puisse parler Ã  votre BigQuery (`dbt init`).
3.  Ã‰crire un modÃ¨le SQL (`stg_stations.sql`) qui lit la table `raw` et crÃ©e une vue propre.

### ğŸ”— Liens utiles
* **La Bible :** [Tuto officiel dbt pour BigQuery](https://docs.getdbt.com/guides/bigquery?step=1) (Suivez "Installation" et "Connect").

### ğŸ†˜ Cheat Codes (BloquÃ© ?)

<details>
<summary>ğŸ‘€ <strong>Cheat 1 : Les commandes d'installation</strong></summary>

Ouvrez le terminal dans VS Code et tapez :
```bash
# Installe la version dbt qui parle Ã  BigQuery
pip install dbt-bigquery

# Lance l'assistant de configuration
dbt init velib_project
```
Note : Si pip ne marche pas, rÃ©installez Python en cochant bien la case "Add to PATH" au dÃ©but.

</details>

<details> <summary>ğŸ‘€ <strong>Cheat 2 : La connexion (profiles.yml)</strong></summary>

Quand dbt init vous demande la mÃ©thode d'authentification :

Choisissez OAuth (c'est le plus simple, Ã§a ouvrira une fenÃªtre Google pour vous connecter).

Projet ID : L'ID de votre projet Google Cloud (pas le nom, l'ID !).

Dataset : le nom du dataset oÃ¹ dbt va Ã©crire (mettez dbt_prod par exemple).

</details>

<details> <summary>ğŸ‘€ <strong>Cheat 3 : Le Code SQL attendu</strong></summary>

CrÃ©ez un fichier dans models/staging/stg_stations.sql :

```sql
/*
   Ce modÃ¨le nettoie la donnÃ©e brute.
   Source : raw_velib.stations
*/

SELECT
    stationCode as station_id,          -- On renomme pour que ce soit clair
    name as station_nom,
    CAST(num_bikes_available AS INT64) as nb_velos_meca, -- On assure que c'est un entier
    CAST(num_ebikes_available AS INT64) as nb_velos_elec,
    (num_bikes_available + num_ebikes_available) as total_velos, -- On crÃ©e une mÃ©trique calculÃ©e
    capacity as capacite_totale
FROM `votre-projet-gcp.raw_velib.stations` -- Remplacez par votre vrai chemin
WHERE is_renting = 'OUI' -- On garde que les stations ouvertes
```
Puis lancez : dbt run

</details>

## Ã‰tape 3 : La Dataviz
La donnÃ©e est propre. Il faut maintenant raconter une histoire avec.

### ğŸ¯ Mission
Aller sur Google Looker Studio.

CrÃ©er une "Source de donnÃ©es" -> Choisir BigQuery -> Choisir la table que dbt vient de crÃ©er (dans dbt_prod).

CrÃ©er une carte de Paris avec des bulles : la taille de la bulle = la capacitÃ© de la station.

# ğŸš€ Phase 2 : L'Industrialisation (Software Engineering)
Vous avez validÃ© la logique mÃ©tier. Mais votre systÃ¨me est nul : il faut uploader le fichier JSON Ã  la main tous les matins. Un ingÃ©nieur automatise.

## Ã‰tape 4 : Python & API
On va remplacer votre clic manuel par un script.

### ğŸ§  C'est quoi le concept ?
API (Application Programming Interface) : C'est comme un serveur dans un resto. Vous (le client) demandez la carte, l'API vous la donne. Vous n'entrez pas en cuisine (la base de donnÃ©es).

JSON : C'est le format standard du Web pour Ã©changer de la donnÃ©e. C'est structurÃ© avec des accolades { "cle": "valeur" }.

### ğŸ¯ Mission
Ã‰crire un script extract_velib.py qui va chercher la donnÃ©e sur le web et nous la montrer.

### ğŸ”— Liens utiles
Apprendre les bases de Requests (Python)

### ğŸ†˜ Cheat Codes
<details> <summary>ğŸ‘€ <strong>Cheat : Le script Python simple</strong></summary>

```python
import requests # Librairie pour faire des requÃªtes HTTP
import json

print("ğŸš€ DÃ©marrage du script...")

url = "[https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json](https://velib-metropole-opendata.smoove.pro/opendata/Velib_Metropole/station_status.json)"

# GET request
response = requests.get(url)

# VÃ©rifier si Ã§a a marchÃ© (Code 200 = OK)
if response.status_code == 200:
    data = response.json()
    print("âœ… DonnÃ©e rÃ©cupÃ©rÃ©e !")
    # Affiche la premiÃ¨re station pour voir Ã  quoi Ã§a ressemble
    print(data['data']['stations'][0])
else:
    print("âŒ Erreur de connexion")
```
</details>

## Ã‰tape 5 : Docker & Airbyte (Le niveau Pro)
Avoir un script Python c'est bien, mais on ne va pas le lancer Ã  la main. On va utiliser un outil d'ingestion moderne : Airbyte, le tout encapsulÃ© dans Docker.

### ğŸ§  C'est quoi le concept ?
Docker (Conteneurisation) : C'est un systÃ¨me qui permet de mettre un logiciel dans une boÃ®te virtuelle. Ã‡a Ã©vite les problÃ¨mes du type "Ã§a marche sur mon ordi mais pas sur le tien".

Airbyte (ELT Tool) : C'est un logiciel open-source qui a des milliers de connecteurs. On lui dit "Prends la donnÃ©e de l'API VÃ©lib" (Source) et "Mets-la dans BigQuery" (Destination). Il gÃ¨re les erreurs, la frÃ©quence, etc.

![Schema](shutterstock.jpeg)

### ğŸ¯ Mission
Installer Docker Desktop.

Installer Airbyte en local (via Docker).

Configurer Airbyte pour qu'il prenne le JSON VÃ©lib et l'envoie dans BigQuery tout seul.

### ğŸ”— Liens utiles (Lisez-les, c'est dur ici !)
Installer Docker Desktop

Comment lancer Airbyte en local (Documentation)

CRUCIAL : CrÃ©er un Service Account Google pour Airbyte (Sans Ã§a, Airbyte n'a pas le droit d'Ã©crire dans BigQuery).

### ğŸ†˜ Cheat Codes (Configuration Airbyte)
<details> <summary>ğŸ‘€ <strong>Cheat : La config Source et Destination</strong></summary>

Une fois Airbyte lancÃ© sur http://localhost:8000 :

Source : Cherchez "File" (Fichier).

Dataset Name : velib_live

URL : L'URL du JSON.

Format : JSON.

Destination : BigQuery.

Project ID : Votre ID GCP.

Dataset Location : EU (ou US selon votre config).

Service Account Key : C'est lÃ  qu'il faut coller le contenu du fichier JSON que vous avez tÃ©lÃ©chargÃ© en crÃ©ant votre Service Account sur GCP (voir lien ci-dessus).

</details>

### ğŸ‰ Le Boss de fin
Si vous avez rÃ©ussi tout Ã§a, vous avez :

Un pipeline d'ingestion automatisÃ© (Airbyte).

Un pipeline de transformation propre (dbt).

Un dashboard qui se met Ã  jour.

C'est littÃ©ralement la Modern Data Stack. Prenez une capture d'Ã©cran de votre "Lineage" dbt (le graphe qui montre les tables) et de votre Dashboard. C'est votre premier projet portfolio.